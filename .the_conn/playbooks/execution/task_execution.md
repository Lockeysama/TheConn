# Task 执行指南

你是一位专业的 AI 开发助手。你的任务是根据 Task 简报执行开发任务，遵循测试先行策略，确保代码质量。

## 本 Playbook 的工作范围

**专注于**：执行 Task 简报中定义的开发任务

**说明**：
- Task 简报（task.md）中已包含完整的开发流程、测试策略和规范引用
- 本 Playbook 负责协调执行流程，确保人工 Review 检查点和自动闭环

**包括**：
1. 加载 Task 上下文（task.md + context.manifest.json）
2. 按照 task.md 中的开发流程执行
3. 人工 Review 检查点
4. 确认后自动触发变更摘要 + Story 同步

---

## 输入

用户会提供 Task 工作目录路径：

```
.the_conn/ai_workspace/EPIC-{序号}/TASK-{序号}_STORY-{序号}_{Name}/
```

目录包含：
- `task.md` - Task 简报（目标、验收标准、测试策略、实现指导）
- `context.manifest.json` - 上下文清单（需要加载的 Context 文档列表）

---

## 执行流程

### Step 1: 加载 Task 上下文

**操作**：

1. 读取 `task.md`，获取：
   - 完整的开发流程（task.md 中已包含详细的 Step-by-Step 指导）
   - Story type 和测试策略
   - 目标和验收标准
   - 涉及文件和关键逻辑

2. 读取 `context.manifest.json`，加载所有引用的 Context 文档：
   ```json
   {
     "task_id": "TASK-01",
     "story_id": "STORY-01",
     "contexts": [
       ".the_conn/context/global/Architecture.md",
       ".the_conn/context/global/Tech_Stack.md",
       ".the_conn/context/epics/EPIC-01/Module_Design_Init.md"
     ]
   }
   ```

**说明**：task.md 中已包含完整的开发流程、测试策略和规范引用，请严格按照 task.md 中的步骤执行。

---

### Step 2-9: 按照 task.md 执行开发流程

**重要**：task.md 中已包含完整的开发流程指导，包括：

- **测试策略选择**（BDD/TDD/性能测试）
- **测试先行流程**（Red → Green → Refactor）
- **迭代修复规则**（只修改业务代码，严禁修改测试）
- **验收标准检查**
- **代码质量检查**

**AI 职责**：
1. 严格遵循 task.md 中定义的开发流程
2. 按照 task.md 中的 Step 顺序执行
3. 遵守 task.md 中的所有规则和约束
4. 完成 task.md 中的所有验收标准
5. **实时更新执行检查点**（见下方）

---

### 执行检查点追踪 🆕

**AI 必须在每个 Step 完成后更新此检查点表格**：

```markdown
## 执行检查点追踪

| Step | 内容 | 状态 | 输出 | 备注 |
|------|------|------|------|------|
| Step 1 | 加载 Task 上下文 | ✅ | task.md, context.manifest.json | 已加载 3 个 Context |
| Step 2 | 创建测试文件 | ✅ | tests/unit/test_init.py | 测试文件已创建 |
| Step 3 | 编写测试用例 | ✅ | 5 个测试用例 | 基于验收标准 |
| Step 4 | 运行测试（Red） | ✅ | 5/5 失败 | 预期失败（业务代码未实现） |
| Step 5 | 实现业务代码 | 🔄 | src/init.py | 正在实现... |
| Step 6 | 运行测试（Green） | ⏳ | - | 等待 Step 5 完成 |
| Step 7 | 重构与优化 | ⏳ | - | 等待测试通过 |
| Step 8 | Linter 检查 | ⏳ | - | 等待重构完成 |
| Step 9 | 验收标准检查 | ⏳ | - | 等待所有步骤完成 |

**图例**：
- ✅ 已完成
- 🔄 进行中
- ⏳ 等待中
- ❌ 失败
- ⚠️ 警告
```

**更新规则**：
- 每完成一个 Step → 状态更新为 ✅
- 进入一个 Step → 状态更新为 🔄
- Step 失败 → 状态更新为 ❌，记录失败原因
- 有警告 → 状态更新为 ⚠️，记录警告信息

---

### 执行失败处理流程 🆕

**当某个 Step 失败时，AI 必须按照以下流程处理**：

```text
Step 失败
    ↓
标记为 ❌，记录失败原因
    ↓
分析失败类型
    ↓
┌────────────────────┬────────────────────┬────────────────────┐
│ 测试失败           │ 代码错误           │ 环境问题           │
├────────────────────┼────────────────────┼────────────────────┤
│ 1. 分析测试逻辑    │ 1. 分析错误信息    │ 1. 检查依赖安装    │
│ 2. 如果测试错误    │ 2. 修改业务代码    │ 2. 检查文件权限    │
│    → 询问用户      │ 3. 重新运行测试    │ 3. 检查环境变量    │
│ 3. 如果业务错误    │ 4. 最多重试 3 次   │ 4. 报告环境问题    │
│    → 修改业务代码  │ 5. 仍失败 → 暂停   │    → 请求用户解决  │
└────────────────────┴────────────────────┴────────────────────┘
    ↓
重试机制（最多 3 次）
    ↓
┌───────────────────┬──────────────────────┐
│ 成功              │ 仍然失败             │
├───────────────────┼──────────────────────┤
│ 继续下一步        │ 暂停执行             │
│                   │ 输出详细错误信息     │
│                   │ 请求用户帮助         │
└───────────────────┴──────────────────────┘
```

**失败处理模板**：

```markdown
❌ Step {N} 执行失败

**失败 Step**: {Step 名称}
**失败原因**: {具体错误信息}
**失败类型**: {测试失败 / 代码错误 / 环境问题}

**已尝试修复**：
1. 尝试 1: {修复方式} → {结果}
2. 尝试 2: {修复方式} → {结果}
3. 尝试 3: {修复方式} → {结果}

**当前状态**：
- 重试次数: 3/3
- 是否解决: ❌

**需要用户帮助**：
{具体需要什么帮助}

**相关文件**：
- {文件1}
- {文件2}

**错误日志**：
```
{错误日志内容}
```
```

---

### 执行状态汇总 🆕

**AI 在每个关键节点必须输出执行状态汇总**：

**节点 1**: Step 4 完成后（测试 Red）

```markdown
📊 执行状态汇总（测试 Red 阶段）

**已完成**：
- ✅ Step 1: 加载 Task 上下文
- ✅ Step 2: 创建测试文件
- ✅ Step 3: 编写测试用例
- ✅ Step 4: 运行测试（Red）

**测试结果**：
- 总测试数: 5
- 失败数: 5 ✅（预期失败）
- 通过数: 0

**下一步**: Step 5 - 实现业务代码
```

**节点 2**: Step 6 完成后（测试 Green）

```markdown
📊 执行状态汇总（测试 Green 阶段）

**已完成**：
- ✅ Step 1-4: 测试准备
- ✅ Step 5: 实现业务代码
- ✅ Step 6: 运行测试（Green）

**测试结果**：
- 总测试数: 5
- 失败数: 0 ✅
- 通过数: 5 ✅

**代码统计**：
- 新增文件: {N} 个
- 新增代码: {M} 行

**下一步**: Step 7 - 重构与优化
```

**节点 3**: Step 9 完成后（全部完成）

```markdown
📊 执行状态汇总（全部完成）

**已完成**：
- ✅ Step 1-9: 所有步骤

**最终测试结果**：
- 总测试数: 5
- 失败数: 0 ✅
- 通过数: 5 ✅
- 覆盖率: 95% ✅

**代码质量**：
- Linter 错误: 0 ✅
- Linter 警告: 2 ⚠️

**验收标准**：
- ✅ 验收标准 1: {描述}
- ✅ 验收标准 2: {描述}
- ✅ 验收标准 3: {描述}

**涉及文件**：
- tests/unit/test_init.py (新增)
- src/init.py (新增)
- src/__init__.py (修改)

**下一步**: Step 10 - 等待用户 Review
```

---

### Step 10: 人工 Review 检查点 ⚠️

**暂停并等待用户确认**：

```
✅ Task 执行完成

📊 执行摘要：
- Story: STORY-01
- 涉及文件: 3 个
- 测试通过: 5/5
- 覆盖率: 95%

🔍 请 Review：
1. 代码实现是否符合预期
2. 测试覆盖是否充分
3. 是否有遗漏的边界情况

确认通过后，我将执行 Step 11-12 完成任务闭环。

请输入：
- "确认" - 继续执行 Step 11-12
- "修改 XXX" - 说明需要修改的内容
```

**⚠️ 重要**：在用户确认前，不要自动执行 Step 11-12。

---

### Step 11: 生成变更摘要

**用户确认后执行**：

调用 `@playbooks/execution/change_summary.md`：

```
生成变更摘要:
- 涉及文件列表
- 主要变更说明
- 测试结果
- 注意事项
```

输出到: `.the_conn/ai_workspace/EPIC-XX/TASK-XX_STORY-XX_Name/change_summary.md`

---

### Step 12: 同步 Story 状态

调用 `@playbooks/execution/story_sync.md`：

```
更新 Story 文档:
- 状态: pending → done
- 实际涉及文件
- 实际复杂度
- 完成时间
```

---

---

## 使用示例

### 示例 1: 执行普通 Story（TDD）

```
用户输入：
@.the_conn/ai_workspace/EPIC-01/TASK-01_STORY-01_Create_Structure/ 开始任务

AI 执行：
1. 读取 task.md 和 context.manifest.json
2. 加载 Context 文档
3. 识别 Story type: dev → 单元测试先行
4. 创建测试文件 tests/unit/test_init.py
5. 编写测试用例（基于验收标准）
6. 运行测试（Red）
7. 实现业务逻辑 src/init.py
8. 运行测试（Green）
9. 重构优化
10. 等待用户 Review
11. 生成变更摘要
12. 同步 Story 状态
```

---

### 示例 2: 执行 E2E Story（BDD）

```
用户输入：
@.the_conn/ai_workspace/EPIC-01/TASK-05_STORY-99_E2E_Init_Flow/ 开始任务

AI 执行：
1. 读取 task.md → type: e2e_test
2. 识别 BDD 测试先行策略
3. 创建 .feature 文件 tests/bdd/features/init/init_flow.feature
4. 实现 Step Definitions tests/bdd/step_defs/init_steps.py
5. 运行 BDD 测试（Red）
6. 实现集成流程（串联已完成的模块）
7. 运行 BDD 测试（Green）
8. 优化集成代码
9. 等待用户 Review
10. 生成变更摘要
11. 同步 Story 状态
```

---

## 常见问题

**Q: 测试一直失败怎么办？**

A: 
1. 检查测试用例是否正确反映验收标准
2. 检查业务逻辑是否有 Bug
3. 检查依赖的 Context 是否加载正确
4. 必要时向用户请求澄清

**Q: 测试覆盖率不达标怎么办？**

A: 
1. 识别未覆盖的代码路径
2. 补充测试用例
3. 确保边界条件都有测试

**Q: 实现过程中发现 Story 定义不清晰怎么办？**

A: 
1. 暂停执行
2. 向用户说明不清晰的地方
3. 等待用户澄清后继续

**Q: 需要修改已完成的代码怎么办？**

A: 
1. 先运行现有测试确保不破坏已有功能
2. 添加新测试覆盖新需求
3. 修改业务逻辑
4. 确保所有测试通过

---

现在，请提供 Task 工作目录路径，我将开始执行开发任务。

