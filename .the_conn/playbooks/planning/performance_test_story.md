# 性能测试 Story 生成指南

你是一位资深的性能工程师。你的任务是为 Feature 或系统生成性能测试 Story，验证系统在压力下的表现。

## ⚠️ 重要：遵守基础公约

**本 Playbook 严格遵守 `@rules/base_rules.md` 中定义的所有基础公约。**

**📋 规范引用**：

本 Playbook 依赖以下规范文件（AI 必须先加载）：
- **基础公约**: `@rules/base_rules.md` - 禁止事项、文件路径约定、质量标准
- **测试策略**: `@rules/test_strategy_rules.md` - 性能测试策略和决策规则

## 本 Playbook 的工作范围

**专注于**：

- ✅ **生成性能测试 Story 文档**：创建性能测试 Story 文件
- ✅ **定义性能指标**：明确响应时间、吞吐量、并发等指标
- ✅ **设计压测场景**：规划负载测试、压力测试、容量测试等场景

---

## 性能测试 Story 类型

### Feature 性能测试

- **用途**：验证单个 Feature 的性能表现
- **ID**：STORY-97
- **触发时机**：Feature 功能开发完成后
- **依赖**：依赖该 Feature 的所有功能 Story

### 系统性能测试

- **用途**：验证整体系统的性能表现
- **ID**：STORY-997
- **触发时机**：Epic 或多个 Feature 完成后
- **依赖**：依赖多个 Feature 的关键 Story

---

## 使用场景

当以下情况发生时，应使用此模板：

1. **明确的性能要求**：需求中有明确的响应时间、吞吐量等指标
2. **高并发场景**：系统需要支持大量并发用户
3. **大数据处理**：涉及大数据量的读写、计算
4. **实时性要求**：有严格的实时性要求
5. **性能瓶颈风险**：已知可能存在性能问题的模块

---

## 命名与格式规范

### ID 命名规则

| 测试范围     | ID 规则   | 示例        | 说明                |
| ------------ | --------- | ----------- | ------------------- |
| Feature 性能 | STORY-97  | `STORY-97`  | Feature 内性能测试  |
| 系统性能     | STORY-997 | `STORY-997` | Epic/系统级性能测试 |

### 文件命名规则

| 测试类型     | 格式                                    | 示例                               |
| ------------ | --------------------------------------- | ---------------------------------- |
| Feature 性能 | `STORY-97_Performance_{FeatureName}.md` | `STORY-97_Performance_DataSync.md` |
| 系统性能     | `STORY-997_Performance_{SystemName}.md` | `STORY-997_Performance_System.md`  |

### 文件路径

```
.the_conn/epics/EPIC-{序号}_{Name}/features/FEAT-{序号}_{Name}/stories/STORY-97_Performance_{Name}.md
```

### Frontmatter 规范

```yaml
---
id: STORY-97
type: perf_test
epic: EPIC-01
feature: FEAT-01
status: pending
created: yyyy-mm-dd
depends_on:
  - STORY-01
  - STORY-02
  - STORY-03
---
```

**字段说明**：

- `type`: 固定为 `perf_test`
- `depends_on`: 列出被测试的所有功能 Story

---

## 输出格式

### Feature 性能测试 Story 模板

```markdown
---
id: STORY-97
type: perf_test
epic: EPIC-{序号}
feature: FEAT-{序号}
status: pending
created: yyyy-mm-dd
depends_on:
  - STORY-01
  - STORY-02
  - STORY-03
---

# Story: Performance_{FeatureName}

## 1. 目标

验证 {Feature 名称} 在生产环境压力下的性能表现，确保满足性能指标要求。

## 2. 性能指标（验收标准）

**核心指标**（必须）：

| 指标类型 | 指标项       | 目标值  | 说明               |
| -------- | ------------ | ------- | ------------------ |
| 响应时间 | P95 响应时间 | < {X}ms | 95% 请求的响应时间 |
| 响应时间 | P99 响应时间 | < {Y}ms | 99% 请求的响应时间 |
| 吞吐量   | TPS/QPS      | ≥ {N}   | 每秒处理请求数     |
| 成功率   | 成功率       | ≥ 99.9% | 请求成功比例       |

**资源使用**（参考）：

| 资源 | 平均使用率 | 峰值使用率 |
| ---- | ---------- | ---------- |
| CPU  | < 60%      | < 80%      |
| 内存 | < {X}GB    | < {Y}GB    |

**基准参考**（根据场景类型）：

- **API 接口**：P95 < 200ms, P99 < 500ms, TPS ≥ 1000
- **数据查询**：P95 < 100ms, P99 < 300ms, QPS ≥ 5000
- **数据写入**：P95 < 500ms, P99 < 1s, TPS ≥ 500
- **批处理**：P95 < 5s, P99 < 10s, 吞吐量 ≥ 10MB/s
- **实时处理**：P95 < 50ms, P99 < 100ms, TPS ≥ 10000

*注：以上为参考值，实际指标应根据业务需求调整*

## 3. 测试场景设计

**核心场景**（必须）：

### 场景 1: 负载测试
- **目的**：验证正常负载下的性能
- **并发**：{N} 用户
- **持续**：{X} 分钟
- **预期**：所有指标满足要求，错误率 < 0.1%

### 场景 2: 压力测试
- **目的**：验证超负载时的表现
- **并发**：{M} 用户（2-3倍正常负载）
- **持续**：{Y} 分钟
- **预期**：系统不崩溃，优雅降级，错误率 < 5%

**可选场景**（按需添加）：

### 场景 3: 容量测试
- **目的**：找到性能拐点
- **方式**：逐步增加并发直到性能下降
- **预期**：确定系统容量上限

### 场景 4: 稳定性测试
- **目的**：验证长时间运行稳定性
- **持续**：≥ 24 小时
- **预期**：无内存泄漏，响应时间稳定

## 4. 测试环境与工具

**环境配置**：
- 服务器配置：{CPU/内存/磁盘规格}
- 网络环境：{带宽/延迟要求}
- 测试数据：{数据量和分布}

**工具推荐**（根据编程语言）：

| 语言    | 负载生成工具                                                                                              | 监控工具             | 说明               |
| ------- | --------------------------------------------------------------------------------------------------------- | -------------------- | ------------------ |
| Go      | [Vegeta](https://github.com/tsenart/vegeta), [k6](https://k6.io/)                                         | Prometheus + Grafana | k6 支持 JS 脚本    |
| Python  | [Locust](https://locust.io/), [pytest-benchmark](https://pytest-benchmark.readthedocs.io/)                | Prometheus + Grafana | Locust 易于编写    |
| Java    | [JMeter](https://jmeter.apache.org/), [Gatling](https://gatling.io/)                                      | JProfiler, VisualVM  | JMeter 功能强大    |
| Node.js | [Artillery](https://artillery.io/), [k6](https://k6.io/)                                                  | Clinic.js, New Relic | Artillery 配置简单 |
| 通用    | [Apache Bench (ab)](https://httpd.apache.org/docs/2.4/programs/ab.html), [wrk](https://github.com/wg/wrk) | Prometheus + Grafana | 适合简单 HTTP 测试 |

**工具选择建议**：
- 快速验证：使用 ab 或 wrk
- 复杂场景：使用 k6 或 Locust
- 企业级：使用 JMeter 或 Gatling

## 5. 实现指导

**测试脚本位置**：
- 性能测试脚本：`tests/performance/{feature_name}/`
- 测试配置：`tests/performance/config/`
- 测试数据：`tests/performance/data/`

**测试执行**：
- 本地测试：快速验证脚本正确性
- 性能环境测试：完整性能测试
- 结果分析：生成性能报告

**监控指标**：
- 应用层：响应时间、吞吐量、错误率
- 系统层：CPU、内存、网络、磁盘
- 数据库：查询时间、连接数、锁等待

## 6. 测试报告

测试完成后，生成简要报告：

- **性能指标**：是否满足目标值
- **瓶颈分析**：主要性能瓶颈（如有）
- **优化建议**：改进方向（如有）
- **结论**：通过/不通过
```

**系统性能测试 Story**：

如需生成系统级（Epic 级）性能测试 Story，使用相同的模板，但：
- ID 使用 STORY-997
- 测试范围扩展到多个 Feature
- 性能指标关注端到端用户旅程

---

## 生成原则

1. **指标明确**：所有性能指标必须可量化、可验证
2. **场景真实**：测试场景尽量贴近生产环境
3. **工具指定**：明确使用的性能测试工具
4. **环境隔离**：使用独立的性能测试环境
5. **报告完整**：测试结束后生成完整的性能报告

---

## 示例

### 示例 1: Feature 性能测试 Story

```markdown
---
id: STORY-97
type: perf_test
epic: EPIC-02
feature: FEAT-01
status: pending
created: 2025-12-16
depends_on:
  - STORY-01
  - STORY-02
  - STORY-03
---

# Story: Performance_DataSync

## 1. 目标

验证数据同步功能在高并发、大数据量场景下的性能表现，确保满足生产环境的性能要求。

## 2. 性能指标（验收标准）

### 响应时间指标

| 操作场景            | 平均响应时间 | P95 响应时间 | P99 响应时间 | 说明          |
| ------------------- | ------------ | ------------ | ------------ | ------------- |
| 单条数据同步        | < 50ms       | < 100ms      | < 200ms      | 小于 10KB     |
| 批量数据同步(100条) | < 500ms      | < 1s         | < 2s         | 总大小 < 1MB  |
| 大数据量同步        | < 5s         | < 10s        | < 15s        | 总大小 < 10MB |

### 吞吐量指标

| 操作场景     | 最小 TPS | 目标 TPS | 说明     |
| ------------ | -------- | -------- | -------- |
| 并发同步请求 | 1000     | 2000     | 峰值容量 |
| 数据处理速度 | 10MB/s   | 20MB/s   | 持续吞吐 |

### 并发能力指标

| 测试类型 | 并发用户数 | 持续时间 | 成功率   |
| -------- | ---------- | -------- | -------- |
| 负载测试 | 500        | 30分钟   | ≥ 99.9%  |
| 压力测试 | 1000       | 15分钟   | ≥ 95%    |
| 容量测试 | 递增至2000 | 2小时    | 找到拐点 |

### 资源使用指标

| 资源类型 | 平均使用率 | 峰值使用率 | 说明     |
| -------- | ---------- | ---------- | -------- |
| CPU      | < 60%      | < 80%      | 4核配置  |
| 内存     | < 4GB      | < 6GB      | 8GB配置  |
| 网络带宽 | < 50Mbps   | < 100Mbps  | 出口带宽 |
| 磁盘 I/O | < 100MB/s  | < 200MB/s  | SSD      |

### 稳定性指标

- [ ] 长时间运行稳定性：持续运行 24 小时无崩溃
- [ ] 内存泄漏检测：长时间运行内存增长 < 10%
- [ ] 错误率：错误率 < 0.1%

## 3. 测试场景设计

### 场景 1: 正常负载测试

**目的**：验证系统在正常负载下的性能表现

**测试步骤**：
1. 准备 500 个并发用户
2. 每个用户执行：
   - 60% 单条数据同步
   - 30% 批量数据同步(100条)
   - 10% 大数据量同步
3. 持续 30 分钟
4. 记录响应时间、吞吐量、资源使用率

**预期结果**：
- 所有性能指标满足要求
- 无错误或错误率 < 0.1%
- 系统资源使用稳定在 60% 以下

### 场景 2: 压力测试

**目的**：验证系统在超出正常负载时的表现

**测试步骤**：
1. 准备 1000 个并发用户（2倍正常负载）
2. 执行相同的操作分布
3. 持续 15 分钟
4. 观察系统响应和稳定性

**预期结果**：
- 响应时间增加但 P99 在 2 倍以内
- 系统不崩溃，优雅降级
- 错误率 < 5%

### 场景 3: 容量测试

**目的**：找到系统的性能拐点

**测试步骤**：
1. 从 100 个并发用户开始
2. 每 10 分钟增加 100 个用户
3. 持续增加直到 P95 响应时间超过 3 倍基准值
4. 记录拐点并发数

**预期结果**：
- 找到系统容量上限（目标 ≥ 2000 并发）
- 确定性能瓶颈点
- 为容量规划提供数据

## 4. 测试环境要求

**环境配置**：
- 应用服务器：4核 CPU，8GB 内存，SSD
- 数据库：8核 CPU，16GB 内存，SSD RAID
- 网络：千兆局域网，100Mbps 外网
- 测试数据：100万条历史数据

**测试工具**：
- 负载生成：Locust
- 监控：Prometheus + Grafana
- APM：Jaeger

## 5. 实现指导

**测试脚本位置**：
- 性能测试脚本：`tests/performance/data_sync/`
- Locust 配置：`tests/performance/data_sync/locustfile.py`
- 监控配置：`tests/performance/data_sync/monitoring/`

**测试执行**：
```bash
# 本地快速测试
locust -f tests/performance/data_sync/locustfile.py --users 10 --spawn-rate 1

# 性能环境完整测试
locust -f tests/performance/data_sync/locustfile.py --users 500 --spawn-rate 10 --run-time 30m
```

**监控指标**：

- 应用层：API 响应时间、吞吐量、错误率
- 系统层：CPU、内存、网络、磁盘
- 数据库：查询时间、连接数、锁等待、慢查询

```

---

现在，请根据用户提供的性能需求生成性能测试 Story。

